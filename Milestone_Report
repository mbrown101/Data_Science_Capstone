---
title: "Data Science Capstone Project Milestone Report"
author: "Michael Brown"
date: "December 12, 2015"
output: html_document
---


## Executive Summary  

The purpose of this report is to describe progress in working with the project data and to provide an status on the development of a prediction algorithm. This report explains the author's exploratory analysis and goals for the app and algorithm. This document is a high level description of the major features of the project data.  The report further summarizes the strategy for creating a prediction algorithm and Shiny app.

The data used in this project is derrived from the HC Corpora which can be further researced at www.corpora.heliohost.org.  The three files used in this project contain blogs, tweets and news content.  


## Task 1: Data Aquisition & Cleaning
I have elected to not remove any profanity as it is an expressive construct of the english language. I have removed numbers however, as they are typically unique to a particular circumstance and not indicative of a future condition.  There certain exceptions to this of course (e.g. 76 Trombones, 101 Maniacs and 5 golden rings)

The following R libraries are used in the data analysis process
```{r message=FALSE , warning=FALSE}
library(ggplot2)
library(R.utils)
library(reader)
library(stringi)
library(quanteda) # for tokenization
library(RColorBrewer)
library(plyr)
library(pander)
```

```{r echo = FALSE}

twitter_location <- '/Users/mike.brown/Documents/Coursera_Data_Science/Capstone/en_US/en_US.twitter.txt'
news_location <- '/Users/mike.brown/Documents/Coursera_Data_Science/Capstone/en_US/en_US.news.txt'
blogs_location <- '/Users/mike.brown/Documents/Coursera_Data_Science/Capstone/en_US/en_US.blogs.txt'
projDir <- '/Users/mike.brown/Documents/Coursera_Data_Science/Capstone/en_US/'
twitterToken <- 'twitterTokenization.txt'
newsToken <- 'newsTokenization.txt'
blogToken <- 'blogTokenization.txt'

```

### File statistics
A basic summary of the file content follows:

File                |  Size on Disk [MB] | Lines   | Word Count
------------------- | ------------------ | ------- | ----------
en_US.twitter.txt   |  167.1             | `r countLines(twitter_location)` |  122
en_US.news.txt      |  205.8             | `r countLines(news_location)` |  121
en_US.blogs.txt     |  210.2             | `r countLines(blogs_location)`  |  1212



```{r message=FALSE , warning=FALSE}

### Read in data

#### Twitter
con <- file(twitter_location, "r") 
twitter.df <- readLines(con , n = -1 , warn = FALSE)
close(con)

### Blogs
con <- file(blogs_location, "r") 
blogs.df <- readLines(con , n = -1 , warn = FALSE)
close(con)

### News
con <- file(news_location, "r") 
news.df <- readLines(con , n = -1 , warn = FALSE)
close(con)

### Twitter DFM with Quanteda
twitter_tfl <- textfile(twitter_location)
twitterCorpus <- corpus(twitter_tfl)
dfm_twitterCorpus <- dfm(twitterCorpus)
topfeatures(dfm_twitterCorpus, 20)
plot(dfm_twitterCorpus, max.words=75, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))

### News DFM with Quanteda
news_tfl <- textfile(news_location)
newsCorpus <- corpus(news_tfl)
dfm_newsCorpus <- dfm(newsCorpus)
topfeatures(dfm_newsCorpus, 20)
plot(dfm_newsCorpus, max.words=75, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))

### Blogs DFM with Quanteda
blogs_tfl <- textfile(blogs_location)
blogsCorpus <- corpus(blogs_tfl)
dfm_blogsCorpus <- dfm(blogsCorpus)
topfeatures(dfm_blogsCorpus, 20)
plot(dfm_blogsCorpus, max.words=75, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))




location = twitter_location

#### Select sample locations 
twitterLines <- countLines(location)

sample_fraction = .00005
samples = sample_fraction * twitterLines
sample_locations <- floor(runif(samples , min = 1 , max = twitterLines + 1)) # select sample locations and convert to integers
sample_locations <- sort(sample_locations) # sort in order
twitter_sent <- NULL

pointer = 0
con <- file(location, "r") 
skip <- sample_locations[1]-1

### Create twitter_sent >> a list of sentences
for (i in 1:length(sample_locations)) {
    readLines(con , skip )                                                                # skip lines  
    twitter_sent[i] <- stri_split_boundaries(readLines(con , 1 ) , type="sentence")
    pointer <- pointer + 1 + skip
    skip = sample_locations[i] - pointer
    }

close(con)        
twitter_sent  <- unlist(twitter_sent)  # remove embedded lists
twitter_sent  <- toLower(twitter_sent) # lowercase all words using quanteda

### Tokenize and create 1-gram
twitter.tok <- NULL
for(i in 1:length(twitter_sent)) {
    twitter.tok[i] <- tokenize(twitter_sent[i], removeNumbers=TRUE, removePunct=TRUE, removeSeparators=TRUE , removeTwitter=FALSE)
}

twitter.tok.1gram.df <- as.data.frame(unlist(twitter.tok))

### 2-gram by quanteda
twitter.tok.2gram <- NULL
for(i in 1:length(twitter_sent)) {
    twitter.tok.2gram[i] <- tokenize(twitter_sent[i], removeNumbers=TRUE, removePunct=TRUE, removeSeparators=TRUE ,         removeTwitter=FALSE , ngrams = 2)
}
twitter.tok.2gram.df <- as.data.frame(unlist(twitter.tok.2gram))
for (i in 1:nrow(twitter.tok.2gram.df )){
    twitter.tok.2gram.df[i,2] <- unlist(strsplit(as.character(twitter.tok.2gram.df[i,1]) , split = '_'))[1]  
    twitter.tok.2gram.df[i,3] <- unlist(strsplit(as.character(twitter.tok.2gram.df[i,1]) , split = '_'))[2] 
    }

### 3-gram by quanteda
twitter.tok.3gram <- NULL
for(i in 1:length(twitter_sent)) {
    twitter.tok.3gram[i] <- tokenize(twitter_sent[i], removeNumbers=TRUE, removePunct=TRUE, removeSeparators=TRUE ,         removeTwitter=FALSE , ngrams = 3)
    }
twitter.tok.3gram.df <- as.data.frame(unlist(twitter.tok.3gram)) 
for (i in 1:nrow(twitter.tok.3gram.df )){
    twitter.tok.3gram.df[i,2] <- unlist(strsplit(as.character(twitter.tok.3gram.df[i,1]) , split = '_'))[1]  
    twitter.tok.3gram.df[i,3] <- unlist(strsplit(as.character(twitter.tok.3gram.df[i,1]) , split = '_'))[2] 
    twitter.tok.3gram.df[i,4] <- unlist(strsplit(as.character(twitter.tok.3gram.df[i,1]) , split = '_'))[3] 
    }


# Save data to file 
#write.csv(tokens, file = paste(projDir , twitterToken))

```


## Exporitory Analysis
#### Basic summary to include:  
  Word counts, line counts and basic data tables
  basic plots, such as histograms to illustrate features of the data
  
  

```{r}


# Number of tokens
token_Count <- length(unlist(twitter.tok))
token_Count

# Frequency of top ten 1-grams
one_gram = count(twitter.tok.1gram.df , 1)
names(one_gram) <- c('term' , 'frequency')
one_gram_sort.df <- one_gram[order(-one_gram[,2]) , ]
one_gram_sort.df[1:10,]

# Frequency of top ten 2-grams
two_gram = count(twitter.tok.2gram.df , 1)
names(two_gram) <- c('term' , 'frequency')
two_gram_sort.df <- two_gram[order(-two_gram[,2]) , ]
two_gram_sort.df[1:10,]

# Frequency of top ten 3-grams
three_gram = count(twitter.tok.3gram.df , 1)
names(three_gram) <- c('term' , 'frequency')
three_gram_sort.df <- three_gram[order(-three_gram[,2]) , ]
three_gram_sort.df[1:10,]
pander(three_gram_sort.df[1:10,])


```
```{r results = "hide"}
# Create DFM
twitterDfm <- dfm(unlist(twitter.tok))
```
```{r}
# Word Frequency
topTwitterWords <- topfeatures(twitterDfm)
topTwitterWords

# Visualize top words
plot(topfeatures(twitterDfm, 100), log = "y", cex = .6, ylab = "Term frequency")
plot(topfeatures(twitterDfm, 100), cex = .6, ylab = "Term frequency")

# Top 10 biword combinations



# Top 10 triword combinations

```

## Modeling

## Prediction

## Creative Exploration
